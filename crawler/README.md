# CS50 Lab 4: Tiny Search Engine Crawler

## Robin Jayaswal, Kyle Dotterrer


### **Overview**

The crawler is a subsystem of the TSE. Crawler crawls a website and retrieves webpages starting with a specified URL. It parses the initial webpage, extracts any embedded href URLs and retrieves those pages, and crawls the pages found at those URLs. The crawler limits itself to maxDepth hops from the seed URL. When the crawling process is complete, the indexing of the collected documents can begin.


### **Program Compilation**

Within the tse/ directory, the default crawler can be compiled with the command: **_make_**

Specifying the following command line will display output of Crawler processes, as well as print out a memory allocation report at the termination of the program: **_make log_**


### **Usage**

crawler seedURL pageDirectory maxDepth

Where:
* seedPage is the page from which web crawling commences
* pageDirectory is the directory path where pages will be stored
* maxDepth is the maximum page depth that the crawler will explore; a maxDepth of 0 tells the crawler to only fetch seedPage, a maxDepth of 1 tells crawler to fetch pages linked from seed, etc

### **Imported Modules / Linked Libraries**
* The libcurl library is used to handle webpage setup, requests, and cleanup
* The memory module (Courtesy of David Kotz) provides program process logging functionalities (See module README for additional information)
* The hashtable module (Courtesy of Kyle Dotterrer) provides basic hashtable creation, insertion, searching, and deletion functionalities (See module README for additional information) 
* The bag module (Courtesy of Kyle Dotterrer) provides basic bag creation, insertion, extraction, and deletion functionalities (See module README for additional information)


### **Assumptions**
* The page directory already exists 
* If the page directory is not located within the current directory, the full path must be included with the directory name
* Valid values for maximum crawl depth range from 0 to 10
* A reasonable size for hashtable of encountered URLs is 1000 slots; this hashtable size is fixed and does not dynamically resize 
* Invalid URLs found by the crawler are quietly ignored; no error is thrown when a curl request fails to retrieve the html for an encountered URL, the page is simply ignored 


### **Limitations** 
* The crawler fetches webpages in LIFO order of URL ecncounter; the program has no prioritization scheme to ensure that 'hot' pages (those possessing a higher number of backlinks) are visited before 'cool' pages
* Program performance may diminish as more hashtable collisions are generated by an exceedingly large number of URLs


### **Exit Codes:**

* 0: Success

#### Argument Errors, 0-10

* 1: Incorrect Argument Count
* 3: seedURL either invalid or not internal
* 4: unable to retrieve page at seed url
* 5: pageDirectory is not writeable
* 7: maxDepth not an integer
* 8: maxDepth outside of range (0 through 10, inclusive)
